# MLOps 

![mlops-architecture](https://cloud.google.com/static/architecture/images/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning-4-ml-automation-ci-cd.svg)

## ðŸ“š MLOps Books

- [Building Machine Learning Powered Applications]()
- [Designing Machine Learning Systems]() 
- [Machine Learning Engineering]()
- [Introducing MLOps]()
- [Practical MLOps]()
- [Kubeflow for Machine Learning]()
- [MLOps Engineering at Scale]()
- [Machine Learning Design Patterns]()

## Resources & Blogs

- https://ml-ops.org
- https://github.com/visenger/awesome-mlops
- https://www.tensorflow.org/tfx
- https://podcasters.spotify.com/pod/show/chaitimedatascience
- https://mlops-coding-course.fmind.dev


## MLOps Tools

- [Data Versioning](): DVC, lakeFS, dolt, Neptune, Delta Lake, Pachyderm 
- [Data & Workflow Management](): Apache Kafka, Apache Airflow, Luigi
- [Feature Stores](): Feast, Databricks Feature Store, AWS Sagemaker Feature Store
- [Model Training](): TensorFlow, PyTorch, Scikit-learn
- [CI/CD for ML](): Kubeflow, MLflow, TFX, MetaFlow
- [Monitoring](): Prometheus, Grafana, ELK Stack, Datadog, custom ML monitoring tools
- [Model Monitoring](): Whylogs, Evidently, Alibi Detect, New Relic
- [CI/CD Tools](): GitHub Actions, Circle CI, Travis CI, Jenkins, Ansible, Terraform, Docker, Kubernetes
- [Model Deployment & Serving](): Flask, Django, Fast API, Tensorflow Serving, KServing(KubeFlow), AWS Sagemaker Endpoints



## Challenges

- `Data Quality`: Ensuring high-quality data for model training
- `Model Drift`: Monitoring and maintaining model performance over time
- `Integration`: Seamlessly integrating machine learning models into existing systems
- `Data Drift`: This occurs when the characteristics of the factors we are studying change because of new behaviors of the users or new prducts being introduced to meet consumer demands.
- `Feature Drift`: Feature properties change over time
- `Input Drift`: Based on the idea thet training Data is an accurate reflection of the type of data seen in production environment.
- `Model Transparency`: XAI, Interpretable Models, Model Introspection, Quantifying Uncertainty, Data provenance, Auditability, Explainability Infrastructure
- `Model Bias`: Certain features of the dataset used for training being more heavily represented/weighted than others.
- `Model Compliance`: GDPR, HIPAA, SOX

## MLOps Architecture

- Level 1: Minimum Viable Architecture
- Level 2: Production Grade MLOps
- Level 3: Enterprise grade MLOps

## Projects

*Below is a list of some MLOPs projects that you can try building inorder to understand the MLOps flow.*

| Sr. No. | Project Name | Description | Dataset Used | Tools | Notebook/Code | Blog |
| -------|--------------|---------------|-------------|----------------| ----| ----|
| 01. | [End-to-End ML Pipeline with MLFlow]() |
| 02. | [Model Deployment Using Flask and Docker]() |
| 03. | [Data Version Control with DVC]() |
| 04. | [CI/CD for ML Models]() |
| 05. | [Model Serving with Kubernetes]() |
| 06. | [Building a Feature Store]() |
| 07. | [Automated Retraining Pipeline with Kubeflow]() |
| 08. | [Monitoring and Logging for ML Models in Production]() |
| 09. | [Multi-Cloud MLOps Pipeline]() |
| 10. | [Advanced Experiment Tracking & Model Management]() |
